{"name":"Pml-course-project","tagline":"","body":"#Written by MRR on 2/11/2016\r\n#Set working directory to folder where data are stored\r\nsetwd(\"C:/Users/Misty Ring-Ramirez/Dropbox/Coursera Courses/Practical Machine Learning Course Project\")\r\n\r\n#Load libraries I anticipate needing\r\nlibrary(caret)\r\nlibrary(rpart)\r\nlibrary(rpart.plot)\r\nlibrary(RColorBrewer)\r\nlibrary(rattle)\r\nlibrary(randomForest)\r\nlibrary(ggplot2)\r\n#Set random seed so results are reproducible\r\nset.seed(223344)\r\n\r\n#Load training and testing data\r\ntrain <- read.csv(\"pml-training.csv\", na.strings=c(\"NA\",\"\"), header=TRUE)\r\ntest <- read.csv(\"pml-testing.csv\", na.strings=c(\"NA\",\"\"), header=TRUE)\r\n\r\n#Partition training dataset into a 60/40 split\r\ninTrain <- createDataPartition(y=train$classe, p=0.6, list=FALSE)\r\nmyTraining <- train[inTrain, ]; myTesting <- train[-inTrain, ]\r\n\r\n#Remove variables with near zero variance\r\nnzv <- nearZeroVar(myTraining, saveMetrics=TRUE)\r\nmyTraining <- myTraining[,nzv$nzv==FALSE]\r\n\r\nnzv<- nearZeroVar(myTesting,saveMetrics=TRUE)\r\nmyTesting <- myTesting[,nzv$nzv==FALSE]\r\n\r\n#Describe dimensions of training and testing dataset\r\ndim(myTraining); dim(myTesting)\r\nsummary(myTraining)\r\n\r\n#Keep the variables I intend to use as features (and the outcome, of course)\r\nmyTraining <- myTraining[c(\"roll_belt\", \"pitch_belt\", \"yaw_belt\", \"total_accel_belt\", \"gyros_belt_x\", \"gyros_belt_y\", \"gyros_belt_z\", \"accel_belt_x\", \"accel_belt_y\", \"accel_belt_z\", \"magnet_belt_x\", \"magnet_belt_y\", \"magnet_belt_z\", \"roll_arm\", \"pitch_arm\", \"yaw_arm\", \"total_accel_arm\", \"gyros_arm_x\", \"gyros_arm_y\", \"gyros_arm_z\", \"accel_arm_x\", \"accel_arm_y\", \"accel_arm_z\", \"magnet_arm_x\", \"magnet_arm_y\", \"magnet_arm_z\", \"roll_dumbbell\", \"pitch_dumbbell\", \"yaw_dumbbell\", \"total_accel_dumbbell\", \"gyros_dumbbell_x\", \"gyros_dumbbell_y\", \"gyros_dumbbell_z\", \"accel_dumbbell_x\", \"accel_dumbbell_y\", \"accel_dumbbell_z\", \"magnet_dumbbell_x\", \"magnet_dumbbell_y\", \"magnet_dumbbell_z\", \"roll_forearm\", \"pitch_forearm\", \"yaw_forearm\", \"total_accel_forearm\", \"gyros_forearm_x\", \"gyros_forearm_y\", \"gyros_forearm_z\", \"accel_forearm_x\", \"accel_forearm_y\", \"accel_forearm_z\", \"magnet_forearm_x\", \"magnet_forearm_y\", \"magnet_forearm_z\", \"classe\")]\r\nmyTesting <- myTesting[c(\"roll_belt\", \"pitch_belt\", \"yaw_belt\", \"total_accel_belt\", \"gyros_belt_x\", \"gyros_belt_y\", \"gyros_belt_z\", \"accel_belt_x\", \"accel_belt_y\", \"accel_belt_z\", \"magnet_belt_x\", \"magnet_belt_y\", \"magnet_belt_z\", \"roll_arm\", \"pitch_arm\", \"yaw_arm\", \"total_accel_arm\", \"gyros_arm_x\", \"gyros_arm_y\", \"gyros_arm_z\", \"accel_arm_x\", \"accel_arm_y\", \"accel_arm_z\", \"magnet_arm_x\", \"magnet_arm_y\", \"magnet_arm_z\", \"roll_dumbbell\", \"pitch_dumbbell\", \"yaw_dumbbell\", \"total_accel_dumbbell\", \"gyros_dumbbell_x\", \"gyros_dumbbell_y\", \"gyros_dumbbell_z\", \"accel_dumbbell_x\", \"accel_dumbbell_y\", \"accel_dumbbell_z\", \"magnet_dumbbell_x\", \"magnet_dumbbell_y\", \"magnet_dumbbell_z\", \"roll_forearm\", \"pitch_forearm\", \"yaw_forearm\", \"total_accel_forearm\", \"gyros_forearm_x\", \"gyros_forearm_y\", \"gyros_forearm_z\", \"accel_forearm_x\", \"accel_forearm_y\", \"accel_forearm_z\", \"magnet_forearm_x\", \"magnet_forearm_y\", \"magnet_forearm_z\", \"classe\")]\r\n\r\n#I plan on using two different prediction methods--decision trees and random forests. \r\n#Prediction with decision trees using rpart\r\nmodFitrp <- rpart(myTraining$classe ~., data=myTraining, method=\"class\")\r\nfancyRpartPlot(modFitrp)\r\n\r\npredictions_rp <- predict(modFitrp, myTesting, type = \"class\")\r\nc_matrix_rp <- confusionMatrix(predictions, myTesting$classe)\r\nc_matrix_rp\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 2045  230   22   83   14\r\n         B   79  911  159  136  146\r\n         C   59  187 1044  102  100\r\n         D   33  103   95  849   97\r\n         E   16   87   48  116 1085\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.7563          \r\n                 95% CI : (0.7467, 0.7658)\r\n    No Information Rate : 0.2845          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.6909          \r\n Mcnemar's Test P-Value : < 2.2e-16       \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9162   0.6001   0.7632   0.6602   0.7524\r\nSpecificity            0.9378   0.9178   0.9308   0.9500   0.9583\r\nPos Pred Value         0.8542   0.6366   0.6997   0.7213   0.8025\r\nNeg Pred Value         0.9657   0.9054   0.9490   0.9345   0.9450\r\nPrevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\nDetection Rate         0.2606   0.1161   0.1331   0.1082   0.1383\r\nDetection Prevalence   0.3051   0.1824   0.1902   0.1500   0.1723\r\nBalanced Accuracy      0.9270   0.7590   0.8470   0.8051   0.8554\r\n\r\n#Prediction with random forests\r\nset.seed(223344)\r\nmodFitrf <- randomForest(classe ~ ., data=myTraining)\r\npredictions_rf <- predict(modFitrf, myTesting, type = \"class\")\r\nc_matrix_rf <- confusionMatrix(modFitrf, myTesting$classe)\r\nc_matrix_rf\r\n\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 2231    4    0    0    0\r\n         B    1 1507   15    0    0\r\n         C    0    7 1351   17    3\r\n         D    0    0    2 1268    1\r\n         E    0    0    0    1 1438\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9935          \r\n                 95% CI : (0.9915, 0.9952)\r\n    No Information Rate : 0.2845          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9918          \r\n Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9996   0.9928   0.9876   0.9860   0.9972\r\nSpecificity            0.9993   0.9975   0.9958   0.9995   0.9998\r\nPos Pred Value         0.9982   0.9895   0.9804   0.9976   0.9993\r\nNeg Pred Value         0.9998   0.9983   0.9974   0.9973   0.9994\r\nPrevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\nDetection Rate         0.2843   0.1921   0.1722   0.1616   0.1833\r\nDetection Prevalence   0.2849   0.1941   0.1756   0.1620   0.1834\r\nBalanced Accuracy      0.9994   0.9951   0.9917   0.9928   0.9985\r\n\r\nplot(modFitrf)\r\n\r\n#It appears that the random forests performed better than the decisions trees, so I continue with that algorithm\r\n#Predicting results on test data\r\npredictions_rf <- predict(modFitrf, myTesting, type = \"class\")\r\npredictions_rf\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}