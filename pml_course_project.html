#Written by MRR on 2/11/2016
#The objective is to predict how much of a particular activity participants do, but they rarely quantify how well they do it.
#Set working directory to folder where data are stored
setwd("C:/Users/Misty Ring-Ramirez/Dropbox/Coursera Courses/Practical Machine Learning Course Project")

#Load libraries I anticipate needing
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(ggplot2)
#Set random seed so results are reproducible
set.seed(223344)

#Load training and testing data
train <- read.csv("pml-training.csv", na.strings=c("NA",""), header=TRUE)
test <- read.csv("pml-testing.csv", na.strings=c("NA",""), header=TRUE)

#Partition training dataset into a 60/40 split, so that I can have both a training and testing dataset.
inTrain <- createDataPartition(y=train$classe, p=0.6, list=FALSE)
myTraining <- train[inTrain, ]; myTesting <- train[-inTrain, ]

#Remove variables with near zero variance, since these are not helpful in making predictions.
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]

nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]

#Describe dimensions of training and testing dataset
dim(myTraining); dim(myTesting)
summary(myTraining)

#Keep the variables I intend to use as features (and the outcome, of course). These are all variables except those describing the data collection process (date collected, for example).
myTraining <- myTraining[c("roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "accel_belt_x", "accel_belt_y", "accel_belt_z", "magnet_belt_x", "magnet_belt_y", "magnet_belt_z", "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm", "gyros_arm_x", "gyros_arm_y", "gyros_arm_z", "accel_arm_x", "accel_arm_y", "accel_arm_z", "magnet_arm_x", "magnet_arm_y", "magnet_arm_z", "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "total_accel_dumbbell", "gyros_dumbbell_x", "gyros_dumbbell_y", "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y", "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y", "magnet_dumbbell_z", "roll_forearm", "pitch_forearm", "yaw_forearm", "total_accel_forearm", "gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z", "accel_forearm_x", "accel_forearm_y", "accel_forearm_z", "magnet_forearm_x", "magnet_forearm_y", "magnet_forearm_z", "classe")]
myTesting <- myTesting[c("roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "accel_belt_x", "accel_belt_y", "accel_belt_z", "magnet_belt_x", "magnet_belt_y", "magnet_belt_z", "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm", "gyros_arm_x", "gyros_arm_y", "gyros_arm_z", "accel_arm_x", "accel_arm_y", "accel_arm_z", "magnet_arm_x", "magnet_arm_y", "magnet_arm_z", "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "total_accel_dumbbell", "gyros_dumbbell_x", "gyros_dumbbell_y", "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y", "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y", "magnet_dumbbell_z", "roll_forearm", "pitch_forearm", "yaw_forearm", "total_accel_forearm", "gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z", "accel_forearm_x", "accel_forearm_y", "accel_forearm_z", "magnet_forearm_x", "magnet_forearm_y", "magnet_forearm_z", "classe")]

#I plan on using two different prediction methods--decision trees and random forests--to compare results and decide which algorithm performs best.
#Prediction with decision trees using rpart
modFitrp <- rpart(myTraining$classe ~., data=myTraining, method="class")
fancyRpartPlot(modFitrp)

predictions_rp <- predict(modFitrp, myTesting, type = "class")
c_matrix_rp <- confusionMatrix(predictions, myTesting$classe)
c_matrix_rp
#As you can see in the confusion matrix below, and with the summary statistics, this algorithm is not doing the best job of predicting class, with an accuracy of only 76% when cross-validated using the testing dataset. Because of this, I move on to another option.

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2045  230   22   83   14
         B   79  911  159  136  146
         C   59  187 1044  102  100
         D   33  103   95  849   97
         E   16   87   48  116 1085

Overall Statistics

               Accuracy : 0.7563
                 95% CI : (0.7467, 0.7658)
    No Information Rate : 0.2845
    P-Value [Acc > NIR] : < 2.2e-16

                  Kappa : 0.6909
 Mcnemar's Test P-Value : < 2.2e-16

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9162   0.6001   0.7632   0.6602   0.7524
Specificity            0.9378   0.9178   0.9308   0.9500   0.9583
Pos Pred Value         0.8542   0.6366   0.6997   0.7213   0.8025
Neg Pred Value         0.9657   0.9054   0.9490   0.9345   0.9450
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2606   0.1161   0.1331   0.1082   0.1383
Detection Prevalence   0.3051   0.1824   0.1902   0.1500   0.1723
Balanced Accuracy      0.9270   0.7590   0.8470   0.8051   0.8554

#Prediction with random forests
set.seed(223344)
modFitrf <- randomForest(classe ~ ., data=myTraining)
predictions_rf <- predict(modFitrf, myTesting, type = "class")
c_matrix_rf <- confusionMatrix(modFitrf, myTesting$classe)
c_matrix_rf

#Based on the accuracy rate of 99%, this algorithm is clearly doing a MUCH better job predicting class. I would move forward with this algorithm.

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2231    4    0    0    0
         B    1 1507   15    0    0
         C    0    7 1351   17    3
         D    0    0    2 1268    1
         E    0    0    0    1 1438

Overall Statistics

               Accuracy : 0.9935
                 95% CI : (0.9915, 0.9952)
    No Information Rate : 0.2845
    P-Value [Acc > NIR] : < 2.2e-16

                  Kappa : 0.9918
 Mcnemar's Test P-Value : NA

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9996   0.9928   0.9876   0.9860   0.9972
Specificity            0.9993   0.9975   0.9958   0.9995   0.9998
Pos Pred Value         0.9982   0.9895   0.9804   0.9976   0.9993
Neg Pred Value         0.9998   0.9983   0.9974   0.9973   0.9994
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2843   0.1921   0.1722   0.1616   0.1833
Detection Prevalence   0.2849   0.1941   0.1756   0.1620   0.1834
Balanced Accuracy      0.9994   0.9951   0.9917   0.9928   0.9985

plot(modFitrf)

#Predicting results on test data
predictions_rf <- predict(modFitrf, myTesting, type = "class")
predictions_rf
